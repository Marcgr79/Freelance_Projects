{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5kSopdSTqB8"
      },
      "source": [
        "### Transformers modelo de lenguaje\n",
        "\n",
        "- Contruir un modelo de lenguaje a partir del texto del Señor de Los\n",
        "Anillos, con una red\n",
        "- Transformer\n",
        "siguiendo el ejemplo de los perceptrones y las redes recurrentes visto\n",
        "en clase. Comparar los resultados entre los ejemplos de clase y los\n",
        "modelos construidos. Puntos a considerar.\n",
        "- Cómo evaluar la calidad de un modelo de lenguaje.\n",
        "- Cuál es la influencia de la tokenización y del encoding en la calidad del\n",
        "modelo.\n",
        "- Qué otros factores influyen.\n",
        "- Tiempo de aprendizaje de los diferentes modelos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "GnFT43EbToy7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from collections import Counter\n",
        "import re\n",
        "import math\n",
        "import time\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYP4kvh3Ube9"
      },
      "source": [
        "### Importamos el texto"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9L7WBkFUPtc"
      },
      "source": [
        "#### Preprocesado de texto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_U0COhLCVRHD"
      },
      "outputs": [],
      "source": [
        "# Preprocess text function\n",
        "def preprocess_text(text, sequence_length):\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text).lower()\n",
        "    tokens = text.split()\n",
        "\n",
        "    sequences = []\n",
        "    for i in range(len(tokens) - sequence_length):\n",
        "        seq = tokens[i:i + sequence_length + 1]\n",
        "        sequences.append(seq)\n",
        "\n",
        "    return sequences\n",
        "\n",
        "# Build vocabulary function\n",
        "def build_vocab(sequences):\n",
        "    all_tokens = [token for seq in sequences for token in seq]\n",
        "    token_counts = Counter(all_tokens)\n",
        "    vocab = {token: idx for idx, (token, _) in enumerate(token_counts.items(), 1)}\n",
        "    vocab['<PAD>'] = 0  # Add padding token\n",
        "    return vocab\n",
        "\n",
        "# Convert sequences to indices function\n",
        "def sequences_to_indices(sequences, vocab):\n",
        "    return [[vocab[token] for token in seq] for seq in sequences]\n",
        "\n",
        "# Read text file function\n",
        "def read_text_file(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        text = file.read()\n",
        "    return text\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "collapsed": true,
        "id": "FUQ8mwtDUzqT"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "vocab_size = 10000\n",
        "embedding_dim = 100\n",
        "nhead = 4\n",
        "num_encoder_layers = 2\n",
        "dim_feedforward = 512\n",
        "learning_rate = 0.001\n",
        "batch_size = 64\n",
        "num_epochs = 10\n",
        "sequence_length = 10\n",
        "file_path = \"/content/LTR.txt\"\n",
        "\n",
        "\n",
        "# Read and preprocess the text file\n",
        "text = read_text_file(file_path)\n",
        "sequences = preprocess_text(text, sequence_length)\n",
        "vocab = build_vocab(sequences)\n",
        "indexed_sequences = sequences_to_indices(sequences, vocab)\n",
        "\n",
        "# Adjust vocab_size according to the actual vocabulary size\n",
        "vocab_size = len(vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyboQOwEUKGA"
      },
      "source": [
        "Construimos la estructura del tranformers, una muy similar a la del ejemplo de clase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Y_GD3845UChF"
      },
      "outputs": [],
      "source": [
        "# Define the dataset class\n",
        "class LanguageDataset(Dataset):\n",
        "    def __init__(self, sequences):\n",
        "        self.sequences = sequences\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.sequences[idx], dtype=torch.long)\n",
        "\n",
        "# Positional Encoding class\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, embedding_dim, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, embedding_dim)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, embedding_dim, 2).float() * (-math.log(10000.0) / embedding_dim))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.transpose(0, 1)  # Transformer expects (sequence_length, batch_size, embedding_dim)\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return x.transpose(0, 1)  # Revert to (batch_size, sequence_length, embedding_dim)\n",
        "\n",
        "# Transformer Language Model class\n",
        "class TransformerLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, nhead, num_encoder_layers, dim_feedforward, max_seq_length):\n",
        "        super(TransformerLanguageModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.positional_encoding = PositionalEncoding(embedding_dim, max_seq_length)\n",
        "        encoder_layers = nn.TransformerEncoderLayer(embedding_dim, nhead, dim_feedforward)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_encoder_layers)\n",
        "        self.fc = nn.Linear(embedding_dim, vocab_size)\n",
        "        self.embedding_dim = embedding_dim\n",
        "\n",
        "    def forward(self, x, src_key_padding_mask=None):\n",
        "        x = self.embedding(x) * math.sqrt(self.embedding_dim)\n",
        "        x = self.positional_encoding(x)\n",
        "        x = x.transpose(0, 1)  # Transformer expects (sequence_length, batch_size, embedding_dim)\n",
        "        output = self.transformer_encoder(x, src_key_padding_mask=src_key_padding_mask)\n",
        "        output = output.transpose(0, 1)\n",
        "        output = self.fc(output)\n",
        "        return output\n",
        "\n",
        "# Training function\n",
        "def train_model(model, data_loader, criterion, optimizer, num_epochs, device):\n",
        "    model.to(device)\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "        for batch in data_loader:\n",
        "            batch = batch.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            inputs = batch[:, :-1]\n",
        "            targets = batch[:, 1:].contiguous().view(-1)\n",
        "\n",
        "            src_key_padding_mask = (inputs == 0)  # Correct mask shape (batch_size, sequence_length)\n",
        "            outputs = model(inputs, src_key_padding_mask=src_key_padding_mask)\n",
        "            outputs = outputs.view(-1, vocab_size)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(data_loader)\n",
        "        print(f'Epoch {epoch+1}, Average Loss: {avg_loss:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_50YvLk5WQvV"
      },
      "source": [
        "Creamos el dataset y el data loader e inicializar el modelo, la loss function y el optimizador (en nuestro caso usamos Adam)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_j6o4dSWOtv",
        "outputId": "626550d1-3a0c-4132-e496-f5bc2ffd07c4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        }
      ],
      "source": [
        "# Create the dataset and data loader\n",
        "dataset = LanguageDataset(indexed_sequences)\n",
        "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Instantiate the model, loss function, and optimizer\n",
        "model_example = TransformerLanguageModel(vocab_size, embedding_dim, nhead, num_encoder_layers, dim_feedforward, sequence_length + 1)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model_example.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y46IZ7FPWfy2"
      },
      "source": [
        "Entrenamos el modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "rJJ-G5oBeBcT"
      },
      "outputs": [],
      "source": [
        "# Determine the device to be used (CPU or GPU)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-WMGwgkdX4g",
        "outputId": "9d4c4c3d-41c3-4e6d-8855-aa9b1b2ab7e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Average Loss: 3.5260\n",
            "Epoch 2, Average Loss: 2.3452\n",
            "Epoch 3, Average Loss: 1.6005\n",
            "Epoch 4, Average Loss: 1.1472\n",
            "Epoch 5, Average Loss: 0.9172\n",
            "Epoch 6, Average Loss: 0.8076\n",
            "Epoch 7, Average Loss: 0.7467\n",
            "Epoch 8, Average Loss: 0.7052\n",
            "Epoch 9, Average Loss: 0.6757\n",
            "Epoch 10, Average Loss: 0.6518\n",
            "Tiempo de entrenamiento del modelo de ejemplo: 621.12 segundos\n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "start_time = time.time()\n",
        "\n",
        "train_model(model_example, data_loader, criterion, optimizer, num_epochs, device=device)\n",
        "\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "\n",
        "print(f'Tiempo de entrenamiento del modelo de ejemplo: {elapsed_time:.2f} segundos')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uu3g_HmAg8uA",
        "outputId": "1633bd98-8014-4bf4-fb1d-4f785a616928"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10.352009479204813\n"
          ]
        }
      ],
      "source": [
        "print(elapsed_time/60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-apZ_kPGhQf5"
      },
      "source": [
        "### Evaluación del modelo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "aW0GxOtihPoX"
      },
      "outputs": [],
      "source": [
        "def calculate_perplexity(model, data_loader, criterion, device):\n",
        "    total_loss = 0.0\n",
        "    total_tokens = 0\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            batch = batch.to(device)\n",
        "            inputs, targets = batch[:, :-1], batch[:, 1:]\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs.view(-1, outputs.size(2)), targets.contiguous().view(-1))\n",
        "            total_loss += loss.item()\n",
        "            total_tokens += targets.numel()\n",
        "\n",
        "    avg_loss = total_loss / len(data_loader)\n",
        "    perplexity = torch.exp(torch.tensor(avg_loss))\n",
        "    return perplexity.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rprXKnV5g-Xo",
        "outputId": "acc5943f-eba1-4385-bcc6-1d6e06c21850"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Perplejidad del modelo: 1.6749\n"
          ]
        }
      ],
      "source": [
        "# Calcular la perplejidad\n",
        "perplexity = calculate_perplexity(model_example, data_loader, criterion, device='cuda')\n",
        "print(f'Perplejidad del modelo: {perplexity:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtooO8fv1g-9"
      },
      "source": [
        "#### Segundo modelo: modificamos parametros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "XJhBLkS71rhc"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "vocab_size = 10000\n",
        "embedding_dim = 100\n",
        "nhead = 4\n",
        "num_encoder_layers = 2\n",
        "dim_feedforward = 512\n",
        "learning_rate = 0.0005\n",
        "batch_size = 64\n",
        "num_epochs = 20\n",
        "sequence_length = 10\n",
        "file_path = \"/content/LTR.txt\"\n",
        "\n",
        "\n",
        "# Read and preprocess the text file\n",
        "text = read_text_file(file_path)\n",
        "sequences = preprocess_text(text, sequence_length)\n",
        "vocab = build_vocab(sequences)\n",
        "indexed_sequences = sequences_to_indices(sequences, vocab)\n",
        "\n",
        "# Adjust vocab_size according to the actual vocabulary size\n",
        "vocab_size = len(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ig2bZET1jad",
        "outputId": "e4c656f6-311e-4588-fb1e-be0226d5bf2d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        }
      ],
      "source": [
        "# Create the dataset and data loader\n",
        "dataset = LanguageDataset(indexed_sequences)\n",
        "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "# Instantiate the model, loss function, and optimizer\n",
        "model_2 = TransformerLanguageModel(vocab_size, embedding_dim, nhead, num_encoder_layers, dim_feedforward, sequence_length + 1)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model_2.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFPU2G7p8wrU",
        "outputId": "87671cd4-af76-4a92-ea55-3c0f1f958e58"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Average Loss: 3.9949\n",
            "Epoch 2, Average Loss: 2.7025\n",
            "Epoch 3, Average Loss: 1.9150\n",
            "Epoch 4, Average Loss: 1.2985\n",
            "Epoch 5, Average Loss: 0.9965\n",
            "Epoch 6, Average Loss: 0.8504\n",
            "Epoch 7, Average Loss: 0.7727\n",
            "Epoch 8, Average Loss: 0.7255\n",
            "Epoch 9, Average Loss: 0.6924\n",
            "Epoch 10, Average Loss: 0.6681\n",
            "Epoch 11, Average Loss: 0.6484\n",
            "Epoch 12, Average Loss: 0.6334\n",
            "Epoch 13, Average Loss: 0.6204\n",
            "Epoch 14, Average Loss: 0.6088\n",
            "Epoch 15, Average Loss: 0.5993\n",
            "Epoch 16, Average Loss: 0.5907\n",
            "Epoch 17, Average Loss: 0.5833\n",
            "Epoch 18, Average Loss: 0.5762\n",
            "Epoch 19, Average Loss: 0.5698\n",
            "Epoch 20, Average Loss: 0.5647\n",
            "Tiempo de entrenamiento del modelo de ejemplo: 1232.86 segundos\n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "start_time = time.time()\n",
        "\n",
        "train_model(model_2, data_loader, criterion, optimizer, num_epochs, device=device)\n",
        "\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "\n",
        "print(f'Tiempo de entrenamiento del modelo de ejemplo: {elapsed_time:.2f} segundos')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VEk7YKlE8z83",
        "outputId": "f00a72e6-471f-4200-c09b-a28f0bd38db9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20.547633866469067\n"
          ]
        }
      ],
      "source": [
        "print(elapsed_time/60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CdVfQFgRKrwm",
        "outputId": "96c560d3-5bd6-4579-8dd3-d288285c75f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Perplejidad del modelo: 1.6264\n"
          ]
        }
      ],
      "source": [
        "# Calcular la perplejidad\n",
        "perplexity = calculate_perplexity(model_2, data_loader, criterion, device='cuda')\n",
        "print(f'Perplejidad del modelo: {perplexity:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kICNExpKwYs"
      },
      "source": [
        "#### Tercer modelo: mínima complejidad posible"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "K-imLOtYKuPv"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "vocab_size = 10000\n",
        "embedding_dim = 50\n",
        "nhead = 2\n",
        "num_encoder_layers = 1\n",
        "dim_feedforward = 256\n",
        "learning_rate = 0.001\n",
        "batch_size = 32\n",
        "num_epochs = 10\n",
        "sequence_length = 10\n",
        "file_path = \"/content/LTR.txt\"\n",
        "\n",
        "\n",
        "# Read and preprocess the text file\n",
        "text = read_text_file(file_path)\n",
        "sequences = preprocess_text(text, sequence_length)\n",
        "vocab = build_vocab(sequences)\n",
        "indexed_sequences = sequences_to_indices(sequences, vocab)\n",
        "\n",
        "# Adjust vocab_size according to the actual vocabulary size\n",
        "vocab_size = len(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "BKPo8ecSK_J6"
      },
      "outputs": [],
      "source": [
        "# Create the dataset and data loader\n",
        "dataset = LanguageDataset(indexed_sequences)\n",
        "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Instantiate the model, loss function, and optimizer\n",
        "model_3 = TransformerLanguageModel(vocab_size, embedding_dim, nhead, num_encoder_layers, dim_feedforward, sequence_length + 1)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model_3.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xw4t7wMoLAme",
        "outputId": "35429089-ebf4-4b9a-e77e-2e4e7782ed57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Average Loss: 5.2373\n",
            "Epoch 2, Average Loss: 4.7035\n",
            "Epoch 3, Average Loss: 4.4938\n",
            "Epoch 4, Average Loss: 4.3526\n",
            "Epoch 5, Average Loss: 4.2415\n",
            "Epoch 6, Average Loss: 4.1525\n",
            "Epoch 7, Average Loss: 4.0709\n",
            "Epoch 8, Average Loss: 4.0046\n",
            "Epoch 9, Average Loss: 3.9433\n",
            "Epoch 10, Average Loss: 3.8891\n",
            "Tiempo de entrenamiento del modelo de ejemplo: 10.85 minutos\n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "start_time = time.time()\n",
        "\n",
        "train_model(model_3, data_loader, criterion, optimizer, num_epochs, device=device)\n",
        "\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "\n",
        "print(f'Tiempo de entrenamiento del modelo de ejemplo: {elapsed_time/60:.2f} minutos')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dbJNH5dL_fm",
        "outputId": "5c4f5ea9-a552-436f-b095-293b879137d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10.853380926450093\n"
          ]
        }
      ],
      "source": [
        "print(elapsed_time/60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XbH7ddauLICh",
        "outputId": "8fcb9433-8246-433d-a668-fe3fbf12dfed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Perplejidad del modelo: 38.5607\n"
          ]
        }
      ],
      "source": [
        "# Calcular la perplejidad\n",
        "perplexity = calculate_perplexity(model_3, data_loader, criterion, device='cuda')\n",
        "print(f'Perplejidad del modelo: {perplexity:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JARvba6q_Bgs"
      },
      "source": [
        "#### Modelo 4: aumento de complejidad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "6zuzLOGqMCrK"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "vocab_size = 10000\n",
        "embedding_dim = 200\n",
        "nhead = 8\n",
        "num_encoder_layers = 3\n",
        "dim_feedforward = 1024\n",
        "learning_rate = 0.001\n",
        "batch_size = 128\n",
        "num_epochs = 10\n",
        "sequence_length = 20\n",
        "file_path = \"/content/LTR.txt\"\n",
        "\n",
        "\n",
        "# Read and preprocess the text file\n",
        "text = read_text_file(file_path)\n",
        "sequences = preprocess_text(text, sequence_length)\n",
        "vocab = build_vocab(sequences)\n",
        "indexed_sequences = sequences_to_indices(sequences, vocab)\n",
        "\n",
        "# Adjust vocab_size according to the actual vocabulary size\n",
        "vocab_size = len(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "ODmIQJxE_PVa"
      },
      "outputs": [],
      "source": [
        "# Create the dataset and data loader\n",
        "dataset = LanguageDataset(indexed_sequences)\n",
        "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Instantiate the model, loss function, and optimizer\n",
        "model_4 = TransformerLanguageModel(vocab_size, embedding_dim, nhead, num_encoder_layers, dim_feedforward, sequence_length + 1)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model_4.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "C8exMKTJ_VaC"
      },
      "outputs": [],
      "source": [
        "# Determine the device to be used (CPU or GPU)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HrV6RV_a_YAf",
        "outputId": "772f38ae-a013-4723-cedb-f7e64ac0f32a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Average Loss: 2.9976\n",
            "Epoch 2, Average Loss: 1.9959\n",
            "Epoch 3, Average Loss: 1.8112\n",
            "Epoch 4, Average Loss: 1.7004\n",
            "Epoch 5, Average Loss: 1.4342\n",
            "Epoch 6, Average Loss: 1.0289\n",
            "Epoch 7, Average Loss: 0.7864\n",
            "Epoch 8, Average Loss: 0.6338\n",
            "Epoch 9, Average Loss: 0.5302\n",
            "Epoch 10, Average Loss: 0.4612\n",
            "Tiempo de entrenamiento del modelo de ejemplo: 23.44 minutos\n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "start_time = time.time()\n",
        "\n",
        "train_model(model_4, data_loader, criterion, optimizer, num_epochs, device=device)\n",
        "\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "\n",
        "print(f'Tiempo de entrenamiento del modelo de ejemplo: {elapsed_time/60:.2f} minutos')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVbqqT89_taj",
        "outputId": "1e057d98-2521-450a-ed76-c542c28bb638"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Perplejidad del modelo: 1.3132\n"
          ]
        }
      ],
      "source": [
        "# Calcular la perplejidad\n",
        "perplexity = calculate_perplexity(model_4, data_loader, criterion, device='cuda')\n",
        "print(f'Perplejidad del modelo: {perplexity:.4f}')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
